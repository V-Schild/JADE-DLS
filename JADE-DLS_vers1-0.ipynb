{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df0723a0-fa8a-45cf-9cf7-739c20376edc",
   "metadata": {},
   "source": [
    "JADE-DLS: Jupyter-based Angular Dependent Evaluator for Dynamic Light Scattering\n",
    "    [vers. 1.0]\n",
    "---\n",
    "different fitting methods can be turned on/off by setting True/False in the corresponding cell"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b63d24d-f30b-4836-b62a-f1772b406c3f",
   "metadata": {},
   "source": [
    "Pre-Processing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df604bc3-9102-4493-aae8-8c6d9a85f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884682a3-6f6a-4a7d-8267-f7a950180c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify folder with data\n",
    "datafolder = r\"C:///*.asc\"\n",
    "\n",
    "#define experiment name or name of the folder is experiment name\n",
    "experiment_name = r\"\"\n",
    "\n",
    "#\n",
    "from preprocessing import get_folder_name\n",
    "if experiment_name: \n",
    "    pass \n",
    "else:\n",
    "    experiment_name = get_folder_name(datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da5f564-2fac-4fee-9cf4-f7895a08ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose between method of catching datafiles\n",
    "#datafiles = glob.glob(os.path.join(datafolder, '*'))\n",
    "datafiles = glob.glob(datafolder)\n",
    "\n",
    "#excluding the *_average.asc-files\n",
    "filtered_files = [f for f in datafiles \n",
    "                  if \"averaged\" not in os.path.basename(f).lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb872b0b-aefa-45b9-96ab-d213050b0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base-data extraction (angle, temperature, wavelength, refractive_index, viscosity)\n",
    "from preprocessing import extract_data\n",
    "\n",
    "all_data = []\n",
    "for file in filtered_files:\n",
    "  extracted_data = extract_data(file)\n",
    "  if extracted_data is not None:\n",
    "     filename = os.path.basename(file)\n",
    "     extracted_data['filename'] = filename  \n",
    "     all_data.append(extracted_data)\n",
    "      \n",
    "if all_data:\n",
    "  df_basedata = pd.concat(all_data, ignore_index=True)\n",
    "  df_basedata.index = df_basedata.index + 1  \n",
    "else:\n",
    "  print(\"No data extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a865d5-8c64-4331-8729-ff7da3506686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate q and q^2 from basedata\n",
    "df_basedata['q'] = abs(((4*np.pi*df_basedata['refractive_index'])/(df_basedata['wavelength [nm]']))*np.sin(np.radians(df_basedata['angle [°]'])/2))\n",
    "df_basedata['q^2'] = (df_basedata['q']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caeb183d-0d92-41ad-8a3e-be1ac2928cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basedata check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check basedata\n",
    "run_cell_basedata = False\n",
    "\n",
    "#\n",
    "if run_cell_basedata:\n",
    "    print(df_basedata)\n",
    "else:\n",
    "    print(\"Basedata check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cdff19-3c3e-4be3-8b4a-2c54d4ebe2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity processing skipped.\n"
     ]
    }
   ],
   "source": [
    "#extract scattering intensity (not required for DLS)\n",
    "perform_intensity_processing = False\n",
    "from intensity import extract_intensity\n",
    "\n",
    "#\n",
    "if perform_intensity_processing:\n",
    "    intensity_data = []\n",
    "    for file in filtered_files:\n",
    "      extracted_intensity = extract_intensity(file)\n",
    "      if extracted_intensity is not None:\n",
    "         filename = os.path.basename(file)\n",
    "         extracted_intensity['filename'] = filename  \n",
    "         intensity_data.append(extracted_intensity)\n",
    "      \n",
    "    if intensity_data:\n",
    "      df_intensity = pd.concat(intensity_data, ignore_index=True)\n",
    "      df_intensity.index = df_intensity.index + 1  \n",
    "    else:\n",
    "      print(\"No data extracted!\")\n",
    "else:\n",
    "    print(\"Intensity processing skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a9d1547-2c6e-4bd1-8bf4-4db6d8cf3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and plot scattering intensity (not required for DLS)\n",
    "if perform_intensity_processing:\n",
    "    from intensity import plot_meancr\n",
    "    df_intensity['MeanCR_corr [kHz]'] = ((df_intensity['meancr0 [kHz]'] + df_intensity['meancr1 [kHz]'])/2/(df_intensity['monitordiode [cps]']*10**(-3))*np.sin(np.radians(df_intensity['angle [°]'])))\n",
    "    plot_meancr(df_intensity, 'angle [°]', 'MeanCR_corr [kHz]')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541960da-a537-4a1b-b323-ed1284e2e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract countrates\n",
    "from preprocessing import find_countrate_row, extract_countrate\n",
    "all_countrates = {}  #dictionary to store the Countrate-dataframes\n",
    "for file in filtered_files:\n",
    "    extracted_countrate = extract_countrate(file)\n",
    "    if extracted_countrate is not None:\n",
    "        filename = os.path.basename(file)\n",
    "        all_countrates[filename] = extracted_countrate\n",
    "\n",
    "#rename columns accordingly\n",
    "new_column_names_for_countrate = {0: 'time [s]', 1: 'detectorslot 1', 2: 'detectorslot 2', 3: 'detectorslot 3', 4: 'detectorslot 4'}\n",
    "all_countrates = {key: df.rename(columns=new_column_names_for_countrate) for key, df in all_countrates.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f41243a-1a30-4737-8f54-6193fe2500f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countrate check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check countrate-dataframes\n",
    "run_cell_countrates = False\n",
    "\n",
    "#\n",
    "if run_cell_countrates:\n",
    "    if all_countrates:  \n",
    "        for filename, df in all_countrates.items():\n",
    "            print(f\"\\nCountrate for {filename}:\")\n",
    "            print(df)\n",
    "    else:\n",
    "        print(\"No countrate extracted!\") \n",
    "else:\n",
    "    print(\"Countrate check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e1f85f-7fd5-40d0-ad24-651b71963e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot countrates skipped.\n"
     ]
    }
   ],
   "source": [
    "#plotting all countrates\n",
    "plot_countrate_graphs = False\n",
    "allow_data_filtering = True \n",
    "\n",
    "#\n",
    "from preprocessing import plot_countrates, cli_countrate_exclusion\n",
    "if plot_countrate_graphs:  \n",
    "    if allow_data_filtering:\n",
    "        filtered_countrates = cli_countrate_exclusion(all_countrates)\n",
    "        if filtered_countrates:\n",
    "            original_countrates = all_countrates\n",
    "            all_countrates = filtered_countrates\n",
    "            print(\"Analysis will continue with the filtered dataset\")\n",
    "    else: #just plot without filtering\n",
    "        plot_countrates(all_countrates, show_indices=False)\n",
    "else:\n",
    "    print(\"Plot countrates skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc621ab-d75d-4713-96dc-74cc0692ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted correlation data for 39 files\n"
     ]
    }
   ],
   "source": [
    "#extract correlation based on filtered countrate data\n",
    "all_correlations = {}\n",
    "#get the original file paths dictionary (assuming it was created earlier)\n",
    "#this maps from filename to full file path\n",
    "file_to_path = {os.path.basename(file): file for file in filtered_files}\n",
    "\n",
    "from preprocessing import find_correlation_row, extract_correlation\n",
    "\n",
    "#process only the files that remain after countrate filtering\n",
    "for filename in all_countrates.keys():\n",
    "    if filename in file_to_path:\n",
    "        file_path = file_to_path[filename]\n",
    "        extracted_correlation = extract_correlation(file_path)\n",
    "        if extracted_correlation is not None:\n",
    "            all_correlations[filename] = extracted_correlation\n",
    "    else:\n",
    "        print(f\"Warning: No matching file found for {filename} in filtered_files\")\n",
    "\n",
    "print(f\"Extracted correlation data for {len(all_correlations)} files\")\n",
    "\n",
    "#rename columns accordingly\n",
    "#  !!! depending on the measurement-settings, some columns may be zero !!!\n",
    "new_column_names_for_correlations = {0: 'time [ms]', 1: 'correlation 1', 2: 'correlation 2', 3: 'correlation 3', 4: 'correlation 4'}\n",
    "all_correlations = {key: df.rename(columns=new_column_names_for_correlations) for key, df in all_correlations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b19bf9-3c05-456b-bffc-2f7ad200a2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check correlation-dataframes\n",
    "run_cell_correlations = False\n",
    "\n",
    "#\n",
    "if run_cell_correlations:\n",
    "    if all_correlations:  \n",
    "        for filename, df in all_correlations.items():\n",
    "            print(f\"\\nCorrelation for {filename}:\")\n",
    "            print(df)\n",
    "    else:\n",
    "        print(\"No correlation extracted!\") \n",
    "else:\n",
    "    print(\"Correlation check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbeba8f4-9c22-44ec-b8e3-a3233551e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot correlation skipped.\n"
     ]
    }
   ],
   "source": [
    "#plotting all normalized correlations (g(2)-1)\n",
    "plot_correlation_graphs = False\n",
    "allow_correlation_filtering = True\n",
    "\n",
    "#\n",
    "from preprocessing import plot_correlations, cli_correlation_exclusion\n",
    "if plot_correlation_graphs:\n",
    "    if allow_correlation_filtering:\n",
    "        filtered_correlations = cli_correlation_exclusion(all_correlations)\n",
    "        if filtered_correlations:\n",
    "            original_correlations = all_correlations\n",
    "            all_correlations = filtered_correlations\n",
    "            print(\"Correlation analysis will continue with the filtered dataset\")\n",
    "    else: #just plot without filtering\n",
    "        plot_correlations(all_correlations, show_indices=False)\n",
    "else:\n",
    "    print(\"Plot correlation skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5bb3715-6ce8-471c-922a-783ad8bebcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 39 correlation datasets for analysis\n",
      "Base data filtered to 39 entries to match correlation data\n"
     ]
    }
   ],
   "source": [
    "#possibility to remove certain data for evaluation\n",
    "from preprocessing import remove_from_data, remove_dataframes\n",
    "#check if filtered data already exists from previous steps\n",
    "if 'filtered_correlations' in locals() and filtered_correlations != all_correlations:\n",
    "    print(\"Using already filtered correlation data from the exclusion process.\")\n",
    "    all_correlations_mod = filtered_correlations\n",
    "else:\n",
    "    # !!! if not explicitly filtered earlier or no changes made, using the existing exclusion method !!!\n",
    "    #datasets to remove (can be left empty if using the exclusion UI)\n",
    "    frame_name = []\n",
    "\n",
    "    all_correlations_mod = remove_dataframes(all_correlations, frame_name)\n",
    "    \n",
    "    #optional: if you want to show which files are being used\n",
    "    print(f\"Using {len(all_correlations_mod)} correlation datasets for analysis\")\n",
    "\n",
    "#update basedata to match the filtered correlation data\n",
    "#get list of files to exclude (those not in the filtered correlations)\n",
    "files_to_exclude = [f for f in df_basedata['filename'] \n",
    "                    if f not in all_correlations_mod.keys()]\n",
    "\n",
    "#filter the basedata\n",
    "df_basedata_mod = remove_from_data(df_basedata, files_to_exclude)\n",
    "\n",
    "#re-index the basedata\n",
    "df_basedata_mod = df_basedata_mod.reset_index(drop=True)  \n",
    "df_basedata_mod.index = df_basedata_mod.index + 1\n",
    "\n",
    "print(f\"Base data filtered to {len(df_basedata_mod)} entries to match correlation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a1d790-fdc0-491c-8c5c-6abc2cd2c4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean temperature [K]</th>\n",
       "      <th>std temperature [K]</th>\n",
       "      <th>sem temperature [K]</th>\n",
       "      <th>mean viscosity [cp]</th>\n",
       "      <th>std viscosity [cp]</th>\n",
       "      <th>sem viscosity [cp]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>297.940634</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.894452</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean temperature [K]  std temperature [K]  sem temperature [K]  \\\n",
       "0            297.940634             0.004605             0.000737   \n",
       "\n",
       "   mean viscosity [cp]  std viscosity [cp]  sem viscosity [cp]  \n",
       "0             0.894452            0.000067            0.000011  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate mean and error for temperature and viscosity (for calculation of R_h)\n",
    "mean_temperature = df_basedata_mod['temperature [K]'].mean()\n",
    "std_temperature = df_basedata_mod['temperature [K]'].std()\n",
    "sem_temperature = df_basedata_mod['temperature [K]'].sem()\n",
    "\n",
    "mean_viscosity = df_basedata_mod['viscosity [cp]'].mean()\n",
    "std_viscosity = df_basedata_mod['viscosity [cp]'].std()\n",
    "sem_viscosity = df_basedata_mod['viscosity [cp]'].sem()\n",
    "\n",
    "#std = standard deviation; sem = standard error\n",
    "df_basedata_stats = pd.DataFrame({\n",
    "    'mean temperature [K]': [mean_temperature], \n",
    "    'std temperature [K]': [std_temperature],\n",
    "    'sem temperature [K]': [sem_temperature],\n",
    "    'mean viscosity [cp]': [mean_viscosity],\n",
    "    'std viscosity [cp]': [std_viscosity],\n",
    "    'sem viscosity [cp]': [sem_viscosity],})\n",
    "df_basedata_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151e39a8-beef-46be-afd8-c9a4b4029a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 2.4398e-19 +/- 1.8548e-23\n",
      "Relative error in c: 0.0076%\n"
     ]
    }
   ],
   "source": [
    "#precalculate c and error for determination of Rh [ c = kb*T/6*pi*eta ] (using standard deviation)\n",
    "from scipy.constants import k #import boltzmann-const.\n",
    "c = (k*df_basedata_stats['mean temperature [K]'])/(6*np.pi*df_basedata_stats['mean viscosity [cp]']*10**(-3))\n",
    "\n",
    "fractional_error_c = np.sqrt((df_basedata_stats['std temperature [K]'] / df_basedata_stats['mean temperature [K]'])**2 + (df_basedata_stats['std viscosity [cp]'] / df_basedata_stats['mean viscosity [cp]'])**2)\n",
    "delta_c = fractional_error_c * c\n",
    "\n",
    "print(f\"c = {c.values[0]:.4e} +/- {delta_c.values[0]:.4e}\")\n",
    "\n",
    "relative_error_c = delta_c / c\n",
    "print(f\"Relative error in c: {relative_error_c.values[0]:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25e9e3b-97ef-49cb-ac1f-938ac4550cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation data from the software are normalized g(2)-1 data\n",
    "#create a new dictionary with dataframes with time [s] and meanvalue of correlation 1 and 2 (has to be adjusted in preprocessing.py for other detectorsettings)\n",
    "from preprocessing import process_correlation_data \n",
    "columns_to_drop = ['time [ms]', 'correlation 1', 'correlation 2', 'correlation 3', 'correlation 4']\n",
    "processed_correlations_1 = process_correlation_data(all_correlations_mod, columns_to_drop)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1978cda-be80-42dc-958f-ccb309c6dd53",
   "metadata": {},
   "source": [
    "CUMULANT-METHODS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e172e4d-515e-4c96-be2e-4ff7dca6ef77",
   "metadata": {},
   "source": [
    "CUMULANT-METHOD A\n",
    "{using cumulant-fit data from ALV-Software}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "994f0e1b-1b03-4d92-a897-b9f1e4e0afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_cumulant_A = False\n",
    "if perform_cumulant_A:\n",
    "    #extract cumulant data only for files in the filtered correlations\n",
    "    from cumulants import extract_cumulants\n",
    "    all_data = []\n",
    "    #create mapping from filename to full path\n",
    "    file_to_path = {os.path.basename(file): file for file in filtered_files}\n",
    "\n",
    "    #only process files that are in the filtered correlations\n",
    "    for filename in all_correlations_mod.keys():\n",
    "        if filename in file_to_path:\n",
    "            file_path = file_to_path[filename]\n",
    "            extracted_cumulants = extract_cumulants(file_path)\n",
    "            if extracted_cumulants is not None:\n",
    "                extracted_cumulants['filename'] = filename  \n",
    "                all_data.append(extracted_cumulants)\n",
    "        else:\n",
    "            print(f\"Warning: No matching file found for {filename} in filtered_files\")\n",
    "\n",
    "    if all_data:\n",
    "        df_extracted_cumulants = pd.concat(all_data, ignore_index=True)\n",
    "        df_extracted_cumulants.index = df_extracted_cumulants.index + 1 \n",
    "    else:\n",
    "        print(\"No cumulant data extracted!\")\n",
    "        #create empty DataFrame to avoid errors in subsequent code\n",
    "        df_extracted_cumulants = pd.DataFrame(columns=['filename', '1st order frequency [1/ms]', \n",
    "                                                 '2nd order frequency [1/ms]', '3rd order frequency [1/ms]',\n",
    "                                                 '2nd order frequency exp param [ms^2]', \n",
    "                                                 '3rd order frequency exp param [ms^2]'])\n",
    "\n",
    "    #if already defined files_to_exclude\n",
    "    if 'files_to_exclude' in locals():\n",
    "        df_extracted_cumulants_mod = remove_from_data(df_extracted_cumulants, files_to_exclude)\n",
    "    else:\n",
    "        #if required to re-filter because we're using all_correlations_mod\n",
    "        df_extracted_cumulants_mod = df_extracted_cumulants\n",
    "\n",
    "    #re-index\n",
    "    df_extracted_cumulants_mod = df_extracted_cumulants_mod.reset_index(drop=True)  \n",
    "    df_extracted_cumulants_mod.index = df_extracted_cumulants_mod.index + 1\n",
    "\n",
    "    print(f\"Extracted cumulant data for {len(df_extracted_cumulants_mod)} files\")\n",
    "\n",
    "    #create a combined dataframe of the modified dataframes (where some files may be removed)\n",
    "    cumulant_method_A_data = pd.merge(df_basedata_mod, df_extracted_cumulants_mod, on = 'filename', how = 'outer')\n",
    "    #re-index\n",
    "    cumulant_method_A_data = cumulant_method_A_data.reset_index(drop=True)  \n",
    "    cumulant_method_A_data.index = cumulant_method_A_data.index + 1\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c282a71-517a-4d19-98ee-2cab744cf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check data of cumulant method A\n",
    "run_cell_cumulant_method_A_data = False\n",
    "\n",
    "#\n",
    "if run_cell_cumulant_method_A_data:\n",
    "    print(cumulant_method_A_data)\n",
    "else:\n",
    "    print(\"Data check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba03a51-6e76-4b9a-91a5-ac317bca21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cumulant data + linear regression to determine diffusion coefficient\n",
    "if perform_cumulant_A:\n",
    "    from cumulants import analyze_diffusion_coefficient\n",
    "    cumulant_method_A_diff = analyze_diffusion_coefficient(\n",
    "        data_df=cumulant_method_A_data, \n",
    "        q_squared_col='q^2', \n",
    "        gamma_cols=['1st order frequency [1/ms]', '2nd order frequency [1/ms]', '3rd order frequency [1/ms]'],\n",
    "        gamma_unit='1/ms', #has to be 1/ms in case of the ALV data, default is 1/s\n",
    "        #x_range=(0, 0.001) #ability to change the range of the fit\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53531596-ad20-40aa-a48f-676cd0b22d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted data for Cumulant Method A not exported.\n"
     ]
    }
   ],
   "source": [
    "#export the plotted data as Cumulant_Method_A_data.txt\n",
    "run_cell_cumulant_method_A_export = False\n",
    "\n",
    "if run_cell_cumulant_method_A_export:\n",
    "    cumulant_method_A_data.to_csv(f'Cumulant_Method_A_data_{experiment_name}.txt', sep='\\t', index=False)\n",
    "else:\n",
    "    print(\"Plotted data for Cumulant Method A not exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c05a96-14ad-4263-9902-ed861c621ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the diffusion-coefficient + error in m^2/s\n",
    "if perform_cumulant_A:\n",
    "    A_diff = pd.DataFrame()\n",
    "    A_diff['D [m^2/s]'] = cumulant_method_A_diff['q^2_coef']*10**(-15)\n",
    "    A_diff['std err D [m^2/s]'] = cumulant_method_A_diff['q^2_se']*10**(-15)\n",
    "    print(A_diff)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87babcde-9813-4e91-8157-42c893a74a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate polydispersity-index\n",
    "#not available for 1st order cumulant fit (for obvious reasons)\n",
    "if perform_cumulant_A:\n",
    "    cumulant_method_A_data['polydispersity_2nd_order'] = cumulant_method_A_data['2nd order frequency exp param [ms^2]']/(cumulant_method_A_data['2nd order frequency [1/ms]'])**2\n",
    "    polydispersity_method_A_2 = cumulant_method_A_data['polydispersity_2nd_order'].mean()\n",
    "    cumulant_method_A_data['polydispersity_3rd_order'] = cumulant_method_A_data['3rd order frequency exp param [ms^2]']/(cumulant_method_A_data['3rd order frequency [1/ms]'])**2\n",
    "    polydispersity_method_A_3 = cumulant_method_A_data['polydispersity_3rd_order'].mean()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c8a42d1-6b7b-4644-85bf-4bdce7c837e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\Desktop\\JADE-DLS-ReleaseVers\\cumulants.py:350: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  plt.ylabel(f'$\\Gamma$ [{gamma_unit}]')\n",
      "C:\\Users\\vince\\Desktop\\JADE-DLS-ReleaseVers\\cumulants.py:352: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  title = 'q$^2$ vs. $\\Gamma$'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rh [nm]</th>\n",
       "      <th>Rh error [nm]</th>\n",
       "      <th>R_squared</th>\n",
       "      <th>Fit</th>\n",
       "      <th>Residuals</th>\n",
       "      <th>PDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 1st order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 2nd order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 3rd order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rh [nm]  Rh error [nm]  R_squared                             Fit  \\\n",
       "0        0              0          0  Rh from 1st order cumulant fit   \n",
       "1        0              0          0  Rh from 2nd order cumulant fit   \n",
       "2        0              0          0  Rh from 3rd order cumulant fit   \n",
       "\n",
       "   Residuals  PDI  \n",
       "0          0  NaN  \n",
       "1          0  0.0  \n",
       "2          0  0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#give results for cumulant method A\n",
    "if perform_cumulant_A:\n",
    "    from cumulants import calculate_cumulant_results_A\n",
    "    method_A_cumulant_result = calculate_cumulant_results_A(A_diff, cumulant_method_A_diff, polydispersity_method_A_2, polydispersity_method_A_3, c, delta_c)\n",
    "else:\n",
    "    from cumulants import create_zero_cumulant_results_A\n",
    "    method_A_cumulant_result = create_zero_cumulant_results_A()\n",
    "method_A_cumulant_result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e16eb647-6322-443a-ac4c-8940da4e0891",
   "metadata": {},
   "source": [
    "CUMULANT-METHOD B\n",
    "{simplest method for Cumulant fit}\n",
    "{using a linear fit method to fit the data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32cc2c1b-29ab-4479-b6f9-9f7893fb31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_cumulant_B = False\n",
    "#calculate sqrt(g2) for the dataframes (=g(2)_mod)\n",
    "#checking if all g(2)-values are positive and dropping all that are below zero (negative values cannot be processed here)\n",
    "#possibility to drop those below zero due to only requiring the data at very short times and negative values only appear near the baseline\n",
    "if perform_cumulant_B:\n",
    "    from cumulants import calculate_g2_B\n",
    "    processed_correlations = calculate_g2_B(processed_correlations_1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e692728-b6fb-406a-be57-3e45f04b8ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check the adapted correlation-data\n",
    "run_cell_process_correlation = False\n",
    "\n",
    "#\n",
    "if run_cell_process_correlation:\n",
    "    print(processed_correlations)\n",
    "else:\n",
    "    print(\"Data check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71993b16-0a26-4fe1-b67e-c47b28396bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot and fit the modified [g(2)-1]-data vs time\n",
    "#keep fit-limits very narrow (depending on fit-results)\n",
    "fit_limits = (0, 0.0002)\n",
    "\n",
    "#fit function (up to 1st moment extension)\n",
    "def fit_function(x, a, b, c):\n",
    "    return 0.5*np.log(a) - b*x + 0.5*c*x**2\n",
    "if perform_cumulant_B:\n",
    "    from cumulants import plot_processed_correlations\n",
    "    cumulant_method_B_fit = plot_processed_correlations(processed_correlations, fit_function, fit_limits)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55418a98-ec5f-4847-a46a-e689bc3344f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a combined dataframe with the basedata\n",
    "if perform_cumulant_B:\n",
    "    cumulant_method_B_data = pd.merge(df_basedata_mod, cumulant_method_B_fit, on = 'filename', how = 'outer')\n",
    "    #re-index\n",
    "    cumulant_method_B_data = cumulant_method_B_data.reset_index(drop=True)  \n",
    "    cumulant_method_B_data.index = cumulant_method_B_data.index + 1 \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6831885e-d01e-4d17-b2a2-8b403affbbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check data of cumulant method B\n",
    "run_cell_cumulant_method_B_data = False\n",
    "\n",
    "#\n",
    "if run_cell_cumulant_method_B_data:\n",
    "    print(cumulant_method_B_data)\n",
    "else:\n",
    "    print(\"Data check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea605bf1-44ad-4641-8c66-a93f4b7f315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove bad fits\n",
    "if perform_cumulant_B:\n",
    "    from cumulants import remove_rows_by_index\n",
    "    indices_input = input(\"Enter row indices to remove (comma-separated): \")\n",
    "    cumulant_method_B_data_mod = remove_rows_by_index(cumulant_method_B_data, indices_input)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f91ab187-f4a9-40d5-9b70-b3e53b4749e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and fit Gamma vs. q^2 for cumulant method B\n",
    "if perform_cumulant_B:\n",
    "    from cumulants import analyze_diffusion_coefficient\n",
    "    cumulant_method_B_diff = analyze_diffusion_coefficient(\n",
    "        data_df=cumulant_method_B_data_mod,\n",
    "        q_squared_col='q^2',\n",
    "        gamma_cols=['b'],\n",
    "        method_names=['Method B'],\n",
    "        #x_range=(0, 0.001) #change the range of the fit\n",
    ")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec99f3ca-0b16-4da8-a207-03cf33e09cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted data for Cumulant Method B not exported.\n"
     ]
    }
   ],
   "source": [
    "#export the plotted data as .txt\n",
    "run_cell_cumulant_method_B_export = False\n",
    "\n",
    "if run_cell_cumulant_method_B_export:\n",
    "    cumulant_method_B_data.to_csv(f'Cumulant_Method_B_data_{experiment_name}.txt', sep='\\t', index=False)  \n",
    "else:\n",
    "    print(\"Plotted data for Cumulant Method B not exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a473b5d-1141-4d98-9621-2bb14b09dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the diffusion-coefficient + error in m^2/s\n",
    "if perform_cumulant_B:\n",
    "    B_diff = pd.DataFrame()\n",
    "    B_diff['D [m^2/s]'] = cumulant_method_B_diff['q^2_coef']*10**(-18)\n",
    "    B_diff['std err D [m^2/s]'] = cumulant_method_B_diff['q^2_se']*10**(-18)\n",
    "    print(B_diff)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35bd9820-e2b9-4952-9752-f19f7bbf58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polydispersity index\n",
    "if perform_cumulant_B:\n",
    "    cumulant_method_B_data_mod['polydispersity'] = cumulant_method_B_data_mod['c']/(cumulant_method_B_data_mod['b'])**2\n",
    "    polydispersity_method_B = cumulant_method_B_data_mod['polydispersity'].mean()\n",
    "    print(polydispersity_method_B)\n",
    "else:\n",
    "    polydispersity_method_B = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ad7409d-3cff-4709-87b8-6753f81f2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulant-Method B not conducted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rh [nm]</th>\n",
       "      <th>Rh error [nm]</th>\n",
       "      <th>R_squared</th>\n",
       "      <th>Fit</th>\n",
       "      <th>Residuals</th>\n",
       "      <th>PDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from linear cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rh [nm]  Rh error [nm]  R_squared                          Fit  Residuals  \\\n",
       "0        0              0          0  Rh from linear cumulant fit          0   \n",
       "\n",
       "   PDI  \n",
       "0    0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results for cumulant_method_B\n",
    "if perform_cumulant_B:\n",
    "    method_B_cumulant_result = pd.DataFrame()\n",
    "    method_B_cumulant_result['Rh [nm]'] = c*(1/B_diff['D [m^2/s]'][0])*10**9\n",
    "    fractional_error_Rh_B = np.sqrt((delta_c / c)**2 + (B_diff['std err D [m^2/s]'][0] / B_diff['D [m^2/s]'][0])**2)\n",
    "    method_B_cumulant_result['Rh error [nm]'] = fractional_error_Rh_B * method_B_cumulant_result['Rh [nm]']\n",
    "    method_B_cumulant_result['R_squared'] = cumulant_method_B_diff['R_squared']\n",
    "    method_B_cumulant_result['Fit'] = 'Rh from linear cumulant fit'\n",
    "    method_B_cumulant_result['Residuals'] = cumulant_method_B_diff['Normality']\n",
    "    method_B_cumulant_result['PDI'] = polydispersity_method_B\n",
    "else:\n",
    "    method_B_cumulant_result = pd.DataFrame({\n",
    "        'Rh [nm]': [0],\n",
    "        'Rh error [nm]': [0],\n",
    "        'R_squared': [0],\n",
    "        'Fit': ['Rh from linear cumulant fit'],\n",
    "        'Residuals': [0],\n",
    "        'PDI': [polydispersity_method_B]\n",
    "    })\n",
    "    print(\"Cumulant-Method B not conducted.\")\n",
    "#\n",
    "method_B_cumulant_result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3e51001-efb0-4d8b-82d5-d3c7b7361eae",
   "metadata": {},
   "source": [
    "CUMULANT-METHOD C\n",
    "{using an iterative nonlinear fit method to fit the data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cd59149-9b2d-468c-8318-6c1911eaaa58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perform_cumulant_C = False\n",
    "if perform_cumulant_C:\n",
    "    #fit-limits as broad as possible\n",
    "    fit_limits = (1e-9, 10) #25ns to 10s should cover the whole range for our instrument\n",
    "\n",
    "    #fit function (up to 4th cumulant)\n",
    "    def fit_function4(x, a, b, c, d, e, f):\n",
    "        inner_term = 1 + 0.5 * c * x**2 - (d * x**3) / 6 + ((e - 3 * c**2) * x**4) / 24\n",
    "        term = f + a * (np.exp(-b * x) * inner_term)**2\n",
    "        return term\n",
    "\n",
    "    #fit function (up to 3rd cumulant) \n",
    "    def fit_function3(x, a, b, c, d, f):\n",
    "        inner_term = 1 + 0.5 * c * x**2 - (d * x**3) / 6\n",
    "        term = f + a * (np.exp(-b * x) * inner_term)**2\n",
    "        return term\n",
    "    \n",
    "    #fit function (up to 2nd cumulant)\n",
    "    def fit_function2(x, a, b, c, f):\n",
    "        inner_term = 1 + 0.5 * c * x**2\n",
    "        term = f + a * (np.exp(-b * x) * inner_term)**2\n",
    "        return term\n",
    "        \n",
    "    #choose fit function\n",
    "    chosen_fit_function = fit_function4\n",
    "    \n",
    "    #possible to choose a parameter adaption strategy and use the adapted parameters instead of the fixed base initial values\n",
    "    adaptive_initial_guesses = True\n",
    "    #if using adaptive parameters, choose adaption strategy:\n",
    "    adaptation_strategy = 'individual'  #adapt initial values for each dataset individually\n",
    "    #adaptation_strategy = 'global' #analyzes all datasets and uses median values for global parameters\n",
    "    #adaptation_strategy = 'representative'  #picks the dataset with the best signal-to-noise ratio and uses its parameters for all datasets\n",
    "    \n",
    "    #define initial guesses\n",
    "    initial_a = 0.8 #basically the starting point (usually around 0.8-0.9)\n",
    "    initial_b = 10000 #basically decay-time -> large positive value in reasonable range\n",
    "    initial_c = 0 #lower c = \"smoother to baseline\" (generelly c>0 for polydisperse samples)\n",
    "    initial_d = 0 #higher d = \"faster to baseline\"\n",
    "    initial_e = 0 #larger e = \"later to baseline\"\n",
    "    initial_f = 0 #baseline is at about 0\n",
    "    base_initial_parameters = [initial_a, initial_b, initial_c, initial_d, initial_e, initial_f] \n",
    "\n",
    "    if adaptive_initial_guesses:\n",
    "        from cumulants_C import get_adaptive_initial_parameters\n",
    "        #get adaptive parameters for all dataframes\n",
    "        initial_parameters = get_adaptive_initial_parameters(\n",
    "            processed_correlations_1,\n",
    "            chosen_fit_function,\n",
    "            base_initial_parameters,\n",
    "            strategy=adaptation_strategy,\n",
    "            verbose=True )\n",
    "    else:\n",
    "        from cumulants_C import get_meaningful_parameters\n",
    "        #get only the meaningful parameters for this function\n",
    "        initial_parameters = get_meaningful_parameters(chosen_fit_function, base_initial_parameters)\n",
    "\n",
    "    #plotting and fitting\n",
    "    from cumulants_C import plot_processed_correlations_iterative\n",
    "    cumulant_method_C_fit = plot_processed_correlations_iterative(processed_correlations_1, chosen_fit_function, fit_limits, initial_parameters, method='lm')\n",
    "    #3 optimization methods available: lm, trf or dogbox\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6505bbd-da85-417e-b7f2-9f00cdbc3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_cumulant_C:\n",
    "    #getting the fit-metric to compare multiple optimization algorithms\n",
    "    from cumulants_C import calculate_mean_fit_metrics\n",
    "    #get mean metrics across all datasets\n",
    "    mean_metrics = calculate_mean_fit_metrics(cumulant_method_C_fit)\n",
    "    #display results\n",
    "    print(f\"Mean R-squared: {mean_metrics['mean_R_squared']:.4f} ± {mean_metrics['std_R_squared']:.4f}\")\n",
    "    print(f\"Mean RMSE: {mean_metrics['mean_RMSE']:.4e} ± {mean_metrics['std_RMSE']:.4e}\")\n",
    "    print(f\"Mean AIC: {mean_metrics['mean_AIC']:.2f} ± {mean_metrics['std_AIC']:.2f}\")\n",
    "    print(f\"Number of successfully fitted datasets: {mean_metrics['num_datasets']}\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fa231a7-fd99-435e-b598-4f08e523ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_cumulant_C:\n",
    "    #create a combined dataframe with the basedata\n",
    "    cumulant_method_C_data = pd.merge(df_basedata_mod, cumulant_method_C_fit, on = 'filename', how = 'outer')\n",
    "    #re-index\n",
    "    cumulant_method_C_data = cumulant_method_C_data.reset_index(drop=True)  \n",
    "    cumulant_method_C_data.index = cumulant_method_C_data.index + 1 \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0f1bac5-74df-48ac-a2be-d4394351451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check skipped.\n"
     ]
    }
   ],
   "source": [
    "#check data of cumulant method C\n",
    "run_cell_cumulant_method_C_data = False\n",
    "\n",
    "#\n",
    "if run_cell_cumulant_method_C_data:\n",
    "    print(cumulant_method_C_data)\n",
    "else:\n",
    "    print(\"Data check skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba995a84-e034-4238-b640-aa9134703103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove bad fits\n",
    "if perform_cumulant_C:\n",
    "    from cumulants import remove_rows_by_index\n",
    "    indices_input = input(\"Enter row indices to remove (comma-separated): \")\n",
    "    cumulant_method_C_data_mod = remove_rows_by_index(cumulant_method_C_data, indices_input)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d13907b-156e-484f-9b7b-10b720ee26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and fit Gamma vs. q^2 for cumulant method C\n",
    "if perform_cumulant_C:\n",
    "    from cumulants import analyze_diffusion_coefficient\n",
    "    cumulant_method_C_diff = analyze_diffusion_coefficient(\n",
    "        data_df=cumulant_method_C_data,\n",
    "        q_squared_col='q^2',\n",
    "        gamma_cols=['best_b'],\n",
    "        method_names=['Method C'],\n",
    "        #x_range=(0, 0.001) #change the range of the fit\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e8bd1b-88ea-474d-8001-7d62bfc9f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted data for Cumulant Method C not exported.\n"
     ]
    }
   ],
   "source": [
    "#export the plotted data as .txt\n",
    "run_cell_cumulant_method_C_export = False\n",
    "\n",
    "if run_cell_cumulant_method_C_export:\n",
    "    cumulant_method_C_data.to_csv(f'Cumulant_Method_C_data_{experiment_name}.txt', sep='\\t', index=False)  \n",
    "else:\n",
    "    print(\"Plotted data for Cumulant Method C not exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4923fdec-2907-43af-8551-945e24f442f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the diffusion-coefficient + error in m^2/s\n",
    "if perform_cumulant_C:\n",
    "    C_diff = pd.DataFrame()\n",
    "    C_diff['D [m^2/s]'] = cumulant_method_C_diff['q^2_coef']*10**(-18)\n",
    "    C_diff['std err D [m^2/s]'] = cumulant_method_C_diff['q^2_se']*10**(-18)\n",
    "    print(C_diff)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90fc28a3-7844-41dc-9a56-f973abb243e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polydispersity index\n",
    "if perform_cumulant_C:\n",
    "    cumulant_method_C_data['polydispersity'] = cumulant_method_C_data['best_c']/(cumulant_method_C_data['best_b'])**2\n",
    "    polydispersity_method_C = cumulant_method_C_data['polydispersity'].mean()\n",
    "    print(polydispersity_method_C)\n",
    "else:\n",
    "    polydispersity_method_C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2b21613-4d71-4ad0-85bc-168e9b8f84a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulant-Method C not conducted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rh [nm]</th>\n",
       "      <th>Rh error [nm]</th>\n",
       "      <th>R_squared</th>\n",
       "      <th>Fit</th>\n",
       "      <th>Residuals</th>\n",
       "      <th>PDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from iterative cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rh [nm]  Rh error [nm]  R_squared                             Fit  \\\n",
       "0        0              0          0  Rh from iterative cumulant fit   \n",
       "\n",
       "   Residuals  PDI  \n",
       "0          0    0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results\n",
    "if perform_cumulant_C:\n",
    "    method_C_cumulant_result = pd.DataFrame()\n",
    "    method_C_cumulant_result['Rh [nm]'] = c*(1/C_diff['D [m^2/s]'][0])*10**9\n",
    "    fractional_error_Rh_C = np.sqrt((delta_c / c)**2 + (C_diff['std err D [m^2/s]'][0] / C_diff['D [m^2/s]'][0])**2)\n",
    "    method_C_cumulant_result['Rh error [nm]'] = fractional_error_Rh_C * method_C_cumulant_result['Rh [nm]']\n",
    "    method_C_cumulant_result['R_squared'] = cumulant_method_C_diff['R_squared']\n",
    "    method_C_cumulant_result['Fit'] = 'Rh from iterative non-linear cumulant fit'\n",
    "    method_C_cumulant_result['Residuals'] = cumulant_method_C_diff['Normality']\n",
    "    method_C_cumulant_result['PDI'] = polydispersity_method_C\n",
    "else:\n",
    "    method_C_cumulant_result = pd.DataFrame({\n",
    "        'Rh [nm]': [0],\n",
    "        'Rh error [nm]': [0],\n",
    "        'R_squared': [0],\n",
    "        'Fit': ['Rh from iterative cumulant fit'],\n",
    "        'Residuals': [0],\n",
    "        'PDI': [polydispersity_method_C]\n",
    "    })\n",
    "    print(\"Cumulant-Method C not conducted.\")\n",
    "#   \n",
    "method_C_cumulant_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c86b34e-090c-4812-a30f-3d2beed55d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rh [nm]</th>\n",
       "      <th>Rh error [nm]</th>\n",
       "      <th>R_squared</th>\n",
       "      <th>Fit</th>\n",
       "      <th>Residuals</th>\n",
       "      <th>PDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 1st order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 2nd order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from 3rd order cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from linear cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rh from iterative cumulant fit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rh [nm]  Rh error [nm]  R_squared                             Fit  \\\n",
       "0        0              0          0  Rh from 1st order cumulant fit   \n",
       "1        0              0          0  Rh from 2nd order cumulant fit   \n",
       "2        0              0          0  Rh from 3rd order cumulant fit   \n",
       "3        0              0          0     Rh from linear cumulant fit   \n",
       "4        0              0          0  Rh from iterative cumulant fit   \n",
       "\n",
       "   Residuals  PDI  \n",
       "0          0  NaN  \n",
       "1          0  0.0  \n",
       "2          0  0.0  \n",
       "3          0  0.0  \n",
       "4          0  0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare all results from cumulant fitting\n",
    "df_all_cumulant_method_results = pd.concat([method_A_cumulant_result, method_B_cumulant_result, method_C_cumulant_result], ignore_index=True)\n",
    "df_all_cumulant_method_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "307106d3-b086-4a9b-a70c-74169c471772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulant-Results-Sheet not exported.\n"
     ]
    }
   ],
   "source": [
    "#export cumulant-results-sheet as .txt\n",
    "run_cell_cumulant_results_sheet = False\n",
    "\n",
    "#\n",
    "if run_cell_cumulant_results_sheet:\n",
    "    cumulant_method_results.to_csv(f'Cumulant_Method_results_{experiment_name}.txt', sep='\\t', index=False) \n",
    "else:\n",
    "    print(\"Cumulant-Results-Sheet not exported.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80f8232c-aa15-4aca-921b-23e80a3fa106",
   "metadata": {},
   "source": [
    "INVERSE-LAPLACIAN-METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ae81d0b-a131-48ca-832d-a4564585acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple NNLS-Fit without additional constraints\n",
    "perform_nnls = False\n",
    "#adjust parameters of NNLS-Fit\n",
    "nnls_params = {\n",
    "    'decay_times': np.logspace(-8, 1, 200), #define space and number of points in which the inverse laplacian problem is fitted\n",
    "    'prominence': 0.05, #a measure how far from the baseline the peak has to be (lower means mmore sensitive peakpicking)\n",
    "    'distance': 1} #minimum distance around the found peaks\n",
    "\n",
    "from regularized import nnls, nnls_all\n",
    "if perform_nnls:\n",
    "    nnls_df = nnls_all(processed_correlations_1, nnls_params)\n",
    "else:\n",
    "    nnls_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c809973-945f-4050-9f8a-28be4fa07c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a combined dataframe with basedata and re-index\n",
    "if perform_nnls:\n",
    "    nnls_data = pd.merge(df_basedata_mod, nnls_df, on='filename', how='outer')\n",
    "    nnls_data = nnls_data.reset_index(drop=True)\n",
    "    nnls_data.index = nnls_data.index + 1\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "766cc9fe-e7b9-4fc9-ae9f-b5bc889e8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove bad fits\n",
    "from cumulants import remove_rows_by_index\n",
    "if perform_nnls:\n",
    "    indices_input = input(\"Enter row indices to remove (comma-separated): \")\n",
    "    nnls_data_mod = remove_rows_by_index(nnls_data, indices_input)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b437f245-798a-4284-936e-9580ffbcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate decay-rates\n",
    "if perform_nnls:\n",
    "    from regularized import calculate_decay_rates\n",
    "    tau_cols = ['tau_1', 'tau_2', 'tau_3'] #can be extended for more peaks if required\n",
    "    nnls_data_mod = calculate_decay_rates(nnls_data_mod, tau_cols)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83664148-84a0-4f7a-8b41-94508a5ca8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data + linear regression to determine diffusion coefficient\n",
    "if perform_nnls:\n",
    "    from cumulants import analyze_diffusion_coefficient\n",
    "    nnls_diff = analyze_diffusion_coefficient(\n",
    "        data_df=nnls_data_mod,\n",
    "        q_squared_col='q^2',\n",
    "        gamma_cols=['gamma_1', 'gamma_2', 'gamma_3'], #can be extended for more than 3 peaks again\n",
    "        #x_range=(0, 0.001) #change the range of the fit\n",
    "    ) \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e79230a8-2f3d-4dd0-8e41-a66175de6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted data for NNLS regression not exported.\n"
     ]
    }
   ],
   "source": [
    "#export the plotted data as .txt\n",
    "run_cell_nnls_export = False\n",
    "\n",
    "#\n",
    "if run_cell_nnls_export:\n",
    "    nnls_data_mod.to_csv(f'nnls_results_{experiment_name}.txt', sep='\\t', index=False) \n",
    "else:\n",
    "    print(\"Plotted data for NNLS regression not exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa39f8e4-c394-4351-8892-f53f24ecd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the diffusion-coefficient + error in m^2/s\n",
    "if perform_nnls:\n",
    "    nnls_diff_results = pd.DataFrame()\n",
    "    nnls_diff_results['D [m^2/s]'] = nnls_diff['q^2_coef']*10**(-18)\n",
    "    nnls_diff_results['std err D [m^2/s]'] = nnls_diff['q^2_se']*10**(-18)\n",
    "    print(nnls_diff_results)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f1c1f58-87a1-4f0c-a27f-acf6343a30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNLS-Fit not performed. Process cancelled.\n"
     ]
    }
   ],
   "source": [
    "#iterate through the multiple fits\n",
    "from IPython.display import display\n",
    "try:\n",
    "    temp_results = []\n",
    "    for i in range(len(nnls_diff_results)):\n",
    "        result = pd.DataFrame()\n",
    "        result['Rh [nm]'] = c * (1 / nnls_diff_results['D [m^2/s]'][i]) * 10**9\n",
    "        fractional_error_Rh = np.sqrt((delta_c / c)**2 + (nnls_diff_results['std err D [m^2/s]'][i] / nnls_diff_results['D [m^2/s]'][i])**2)\n",
    "        result['Rh error [nm]'] = fractional_error_Rh * result['Rh [nm]']\n",
    "        result['R_squared'] = [nnls_diff['R_squared'][i]]\n",
    "        result['Fit'] = [f'Rh from fit tau_{i+1}']\n",
    "        result['Residuals'] = [nnls_diff['Normality'][i]]\n",
    "        temp_results.append(result)\n",
    "    df_final_nnls_results = pd.concat(temp_results, ignore_index=True)\n",
    "    display(df_final_nnls_results)\n",
    "except NameError:\n",
    "    print(\"NNLS-Fit not performed. Process cancelled.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f028df3a-1f92-4b59-a18c-a2bc7f2e96e4",
   "metadata": {},
   "source": [
    "REGULARIZED FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01aac7d3-327e-4368-94f6-1af52babe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-analysis skipped.\n"
     ]
    }
   ],
   "source": [
    "#predetermine a suitable alpha-value by analyzing a random set of data with different alpha-values\n",
    "analyze_alpha = False\n",
    "from regularized import nnls_reg_simple, analyze_random_datasets_grid\n",
    "if analyze_alpha:\n",
    "    base_params = {\n",
    "    'decay_times': np.logspace(-8, 1, 200), #define space and numer of points in which the inverse laplacian problem is fitted\n",
    "    'prominence': 0.01, #a measure how far from the baseline the peak has to be (lower means mmore sensitive peakpicking)\n",
    "    'distance': 1} #minimum distance around the found peaks\n",
    "\n",
    "    fig, selected_datasets = analyze_random_datasets_grid(\n",
    "    processed_correlations_1, \n",
    "    num_datasets=3, #number of randomly selected datasets\n",
    "    base_nnls_params=base_params, \n",
    "    nnls_reg_simple_function=nnls_reg_simple, #also possible with nnls_reg\n",
    "    alpha_range=(0.1, 1), #set alpha range (logarithmic spacing between values)\n",
    "    num_alphas=5) #number of alphas in range \n",
    "else:\n",
    "    print(\"alpha-analysis skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebb18d08-3ec0-4bf4-bce7-cd061d35662c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#regularized fit (NNLS-fit with Tikhonov-Phillips regulation)\n",
    "regularized_fit = False\n",
    "#adjust parameters of NNLS-Reg-Fit\n",
    "nnls_reg_params = {\n",
    "    'decay_times': np.logspace(-8, 1, 200), #define space and number of points in which the inverse laplacian problem is fitted\n",
    "    'prominence': 0.01, #a measure how far from the baseline the peak has to be (lower means mmore sensitive peakpicking)\n",
    "    'distance': 1, #minimum distance around the found peaks\n",
    "    'alpha': 0.5, #alpha-value\n",
    "    'normalize': True, #enable for normalization           \n",
    "    'sparsity_penalty': 0, #>0 introduces a sparsity penalty\n",
    "    'enforce_unimodality': False #if true, enforces the result to be one peak, can be used for monomodal samples as comparison to cumulant analysis\n",
    "}\n",
    "if regularized_fit:\n",
    "    from regularized import nnls_reg, nnls_reg_all\n",
    "    nnls_reg_df, full_results = nnls_reg_all(processed_correlations_1, nnls_reg_params)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "196e2611-9282-44bf-8e61-69179a107b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a combined dataframe with basedata and re-index\n",
    "if regularized_fit:\n",
    "    nnls_reg_data = pd.merge(df_basedata_mod, nnls_reg_df, on='filename', how='outer')\n",
    "    nnls_reg_data = nnls_reg_data.reset_index(drop=True)\n",
    "    nnls_reg_data.index = nnls_reg_data.index + 1\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72cc9022-2123-4ac3-9142-e6cc8d42598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove bad fits\n",
    "if regularized_fit:\n",
    "    from cumulants import remove_rows_by_index\n",
    "    indices_input = input(\"Enter row indices to remove (comma-separated): \")\n",
    "    nnls_reg_data_mod = remove_rows_by_index(nnls_reg_data, indices_input)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5af3e716-4730-4b05-b839-976e7241a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate decay-rates\n",
    "if regularized_fit:\n",
    "    from regularized import calculate_decay_rates\n",
    "    tau_cols = ['tau_1', 'tau_2', 'tau_3', 'tau_4'] #can be extended for more peaks if required\n",
    "    nnls_reg_data_mod = calculate_decay_rates(nnls_reg_data_mod, tau_cols)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08a68769-dcc3-410f-bc9d-5b0c8466221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data + linear regression to determine diffusion coefficient\n",
    "if regularized_fit:\n",
    "    from cumulants import analyze_diffusion_coefficient\n",
    "    nnls_reg_diff = analyze_diffusion_coefficient(\n",
    "        data_df=nnls_reg_data_mod,\n",
    "        q_squared_col='q^2',\n",
    "        gamma_cols=['gamma_1', 'gamma_2', 'gamma_3'], #can be extended for more peaks again\n",
    "        #x_range=(0, 0.001) #change the range of the fit\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "525b4aaa-08fe-434d-8592-134c10eb9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted data for regularized NNLS regression not exported.\n"
     ]
    }
   ],
   "source": [
    "#export the plotted data as .txt\n",
    "run_cell_nnls_reg_export = False\n",
    "\n",
    "#\n",
    "if run_cell_nnls_reg_export:\n",
    "    nnls_reg_data_mod.to_csv(f'nnls_reg_results_{experiment_name}.txt', sep='\\t', index=False) \n",
    "else:\n",
    "    print(\"Plotted data for regularized NNLS regression not exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29701001-72ff-49db-beac-7dad43d30501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the diffusion-coefficient + error in m^2/s\n",
    "if regularized_fit:\n",
    "    nnls_reg_diff_results = pd.DataFrame()\n",
    "    nnls_reg_diff_results['D [m^2/s]'] = nnls_reg_diff['q^2_coef']*10**(-18)\n",
    "    nnls_reg_diff_results['std err D [m^2/s]'] = nnls_reg_diff['q^2_se']*10**(-18)\n",
    "    print(nnls_reg_diff_results)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "243e23e8-99df-49ee-ad48-073f44210faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Fit not performed. Process cancelled.\n"
     ]
    }
   ],
   "source": [
    "#iterate through the multiple fits\n",
    "from IPython.display import display\n",
    "try:\n",
    "    temp_results = []\n",
    "    for i in range(len(nnls_reg_diff_results)):\n",
    "        result = pd.DataFrame()\n",
    "        result['Rh [nm]'] = c * (1 / nnls_reg_diff_results['D [m^2/s]'][i]) * 10**9\n",
    "        fractional_error_Rh = np.sqrt((delta_c / c)**2 + (nnls_reg_diff_results['std err D [m^2/s]'][i] / nnls_reg_diff_results['D [m^2/s]'][i])**2)\n",
    "        result['Rh error [nm]'] = fractional_error_Rh * result['Rh [nm]']\n",
    "        result['R_squared'] = [nnls_reg_diff['R_squared'][i]]\n",
    "        result['Fit'] = [f'Rh from fit tau_{i+1}']\n",
    "        result['Residuals'] = [nnls_reg_diff['Normality'][i]]\n",
    "        temp_results.append(result)\n",
    "    df_final_nnls_reg_results = pd.concat(temp_results, ignore_index=True)\n",
    "    display(df_final_nnls_reg_results)\n",
    "except NameError:\n",
    "    print(\"Regularized Fit not performed. Process cancelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d9f19a1-29fe-4bdc-8298-1fa55ca5ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare different angles of the regularized fit\n",
    "compare_results_reg = False\n",
    "\n",
    "if compare_results_reg:\n",
    "    from regularized import plot_distributions\n",
    "    plot_distributions(full_results, nnls_reg_params, nnls_reg_data, \n",
    "                   angles=[30,90,150], #choose angles to plot\n",
    "                   measurement_mode='average', #average: plots the average of that angle, first: plots first distribution of that angle, all: plots all distributions \n",
    "                   convert_to_radius=True, #plot Rh instead of decay time\n",
    "                   figsize=(8, 6), #figure size\n",
    "                   title=\"Distribution Comparison\" #title\n",
    "                  )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0ad17-4157-4246-9827-9c967ab1e773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
